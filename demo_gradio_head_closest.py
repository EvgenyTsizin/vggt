#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import sys
import gc
import cv2
import glob
import shutil
import numpy as np
import torch
import trimesh
import gradio as gr
from datetime import datetime

sys.path.append("vggt/")

from vggt.models.vggt import VGGT
from vggt.utils.load_fn import load_and_preprocess_images
from vggt.utils.pose_enc import pose_encoding_to_extri_intri
from vggt.utils.geometry import unproject_depth_map_to_point_map

device = "cuda" if torch.cuda.is_available() else "cpu"

print("Initializing and loading VGGT model...")
model = VGGT()
_URL = "https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt"
model.load_state_dict(torch.hub.load_state_dict_from_url(_URL))
model.eval()
model = model.to(device)


# -------------------------------------------------------------------------
# 1. CORE LOGIC: CLOSEST CAMERA FILTERING
# -------------------------------------------------------------------------

def get_camera_centers_world(extrinsics):
    """
    Calculates the World Position of each camera.
    Extrinsics E are [R|t] mapping World -> Camera.
    Camera Center C in World = -R^T * t.
    """
    S = extrinsics.shape[0]
    centers = []
    for i in range(S):
        R = extrinsics[i, :3, :3]
        t = extrinsics[i, :3, 3]
        # C = -R_transpose * t
        C = -np.dot(R.T, t)
        centers.append(C)
    return np.array(centers) # (S, 3)

def filter_points_by_closest_camera(predictions, rgb_stack_full):
    """
    Implements 'Closest Camera Ownership':
    1. Lifts all points from all cameras.
    2. For every single point P (generated by Cam A):
       - Calculate distance to Cam A.
       - Calculate distance to ALL other cameras.
       - IF dist(P, Cam A) is the minimum -> KEEP.
       - ELSE (some other Cam B is closer) -> DISCARD.
    """
    print("Running Closest Camera Filtering...")
    
    WP = predictions.get("world_points_from_depth") # (S, H, W, 3)
    E = predictions.get("extrinsic")
    
    S, H, W = WP.shape[:3]
    
    # 1. Resize images to match point cloud resolution (for coloring)
    rgb_stack = []
    for img in rgb_stack_full:
        rgb_stack.append(cv2.resize(img, (W, H), interpolation=cv2.INTER_LINEAR))
    rgb_stack = np.array(rgb_stack) # (S, H, W, 3)

    # 2. Get Camera Centers in World Space
    camera_centers = get_camera_centers_world(E) # (S, 3)

    # 3. Flatten Data for Vectorized Processing
    # We need to track which camera generated which point.
    
    # Create Source IDs: [0, 0, ... 0, 1, 1, ... 1, ...]
    source_ids = np.repeat(np.arange(S), H * W)
    
    # Flatten Points and Colors
    flat_pts = WP.reshape(-1, 3)          # (N, 3)
    flat_rgb = rgb_stack.reshape(-1, 3)   # (N, 3)
    
    # 4. Pre-filter Invalid Points (NaN/Inf or Depth=0) to save memory/time
    #    (Points at 0,0,0 or infinity shouldn't be processed)
    valid_mask = np.isfinite(flat_pts).all(axis=1) & (np.linalg.norm(flat_pts, axis=1) > 0.01)
    
    active_pts = flat_pts[valid_mask]
    active_rgb = flat_rgb[valid_mask]
    active_src = source_ids[valid_mask]
    
    num_points = len(active_pts)
    print(f"Total valid points lifted: {num_points}")
    
    if num_points == 0:
        return None, None

    # 5. Distance Calculation (Vectorized)
    # We need dist(Point_i, Cam_k) for all k.
    # To avoid a huge (N, S, 3) tensor, we loop over cameras to build the (N, S) distance matrix columns.
    
    dists_to_cams = np.zeros((num_points, S), dtype=np.float32)
    
    for cam_idx in range(S):
        # Center of camera k
        C = camera_centers[cam_idx] # (3,)
        
        # Vectors from all points to this camera center
        vecs = active_pts - C # (N, 3)
        
        # Euclidean distance
        dists = np.linalg.norm(vecs, axis=1) # (N,)
        dists_to_cams[:, cam_idx] = dists

    # 6. The "Ownership" Check
    # Find which camera index is the closest for every point
    closest_cam_indices = np.argmin(dists_to_cams, axis=1) # (N,)
    
    # Keep point IF its creator (active_src) IS the closest camera
    ownership_mask = (closest_cam_indices == active_src)
    
    filtered_pts = active_pts[ownership_mask]
    filtered_rgb = active_rgb[ownership_mask]
    
    print(f"Points kept after filtering: {len(filtered_pts)} ({(len(filtered_pts)/num_points)*100:.2f}%)")
    
    return filtered_pts, filtered_rgb

def create_filtered_glb(predictions, target_dir):
    # Load raw images
    image_names = sorted(glob.glob(os.path.join(target_dir, "images", "*")))
    rgb_stack_full = []
    for p in image_names:
        rgb_stack_full.append(cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB))
    
    # Run the Logic
    final_pts, final_rgb = filter_points_by_closest_camera(predictions, rgb_stack_full)
    
    if final_pts is None or len(final_pts) == 0:
        return None

    # Normalize Colors for Trimesh (0..1) or keep uint8? Trimesh handles uint8.
    # But let's verify. Usually trimesh expects uint8 for visual.
    
    # Create Point Cloud
    pcd = trimesh.points.PointCloud(vertices=final_pts, colors=final_rgb)
    
    scene = trimesh.Scene([pcd])
    return scene

# -------------------------------------------------------------------------
# 2. STANDARD INFERENCE
# -------------------------------------------------------------------------
def run_model(target_dir):
    print(f"Processing: {target_dir}")
    image_names = sorted(glob.glob(os.path.join(target_dir, "images", "*")))
    images = load_and_preprocess_images(image_names).to(device)
    
    with torch.no_grad():
        with torch.cuda.amp.autocast(dtype=torch.float16):
            pred = model(images)
    
    ext, intr = pose_encoding_to_extri_intri(pred["pose_enc"], images.shape[-2:])
    pred["extrinsic"] = ext
    pred["intrinsic"] = intr
    
    for k, v in pred.items():
        if isinstance(v, torch.Tensor):
            pred[k] = v.detach().cpu().numpy().squeeze(0) if v.ndim > 0 and v.shape[0]==1 else v.detach().cpu().numpy()
            
    try:
        d_map = pred["depth"]
        # Ensure 4D shape for geometry util
        if d_map.ndim == 3: d_map = d_map[..., None]
        
        pred["world_points_from_depth"] = unproject_depth_map_to_point_map(
            d_map, pred["extrinsic"], pred["intrinsic"]
        )
    except Exception as e:
        print("Warning:", e)
        pred["world_points_from_depth"] = None
        
    return pred

def handle_upload(vid, imgs):
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    td = f"input_{ts}"
    td_img = os.path.join(td, "images")
    os.makedirs(td_img, exist_ok=True)
    
    if imgs:
        for f in imgs:
            path = f.name if hasattr(f, 'name') else f
            shutil.copy(path, os.path.join(td_img, os.path.basename(path)))
    elif vid:
        cap = cv2.VideoCapture(vid)
        fps = cap.get(cv2.CAP_PROP_FPS)
        interval = max(1, int(fps)) 
        cnt = 0
        saved = 0
        while True:
            ret, frame = cap.read()
            if not ret: break
            if cnt % interval == 0:
                cv2.imwrite(os.path.join(td_img, f"{saved:05d}.png"), frame)
                saved += 1
            cnt += 1
    return td


# -------------------------------------------------------------------------
# UI
# -------------------------------------------------------------------------
def process_pipeline(target_dir):
    if not target_dir: return None, "Upload first."
    
    # 1. Inference
    preds = run_model(target_dir)
    
    # 2. Filter & Display
    # Logic: Closest Camera Ownership -> GLB Point Cloud
    scene = create_filtered_glb(preds, target_dir)
    
    if scene is None:
        return None, "No points found."
        
    out_path = os.path.join(target_dir, "filtered_closest_cam.glb")
    scene.export(file_obj=out_path)
    
    return out_path, "Done. Point Cloud filtered by Closest Camera."


with gr.Blocks() as demo:
    gr.Markdown("# VGGT: Closest-Camera Filtering")
    gr.Markdown("Logic: Keeps a 3D point **only** if the camera that generated it is geographically closer to the point than any other camera. Eliminates redundancy and 'spraying' walls.")
    
    t_dir = gr.State()
    
    with gr.Row():
        with gr.Column(scale=1):
            in_vid = gr.Video(label="Input Video")
            in_imgs = gr.File(label="Input Images", file_count="multiple")
            btn_upload = gr.Button("1. Upload")
            btn_run = gr.Button("2. Filter & View", variant="primary")
            log = gr.Textbox(label="Status")
            
        with gr.Column(scale=2):
            out_3d = gr.Model3D(label="Filtered Point Cloud", height=700)
            
    def upload(v, i):
        td = handle_upload(v, i)
        return td, f"Ready: {td}"

    btn_upload.click(upload, [in_vid, in_imgs], [t_dir, log])
    btn_run.click(process_pipeline, [t_dir], [out_3d, log])

demo.launch(share=True)
